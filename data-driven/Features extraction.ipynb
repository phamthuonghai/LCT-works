{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## \"Normal\" features\n",
    "* Number of words with all capital characters\n",
    "* Number of capital characters\n",
    "* Number of characters\n",
    "* Number of words\n",
    "* Punctuations (Exclamation mark, Question mark)\n",
    "* Elongated words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092\n"
     ]
    }
   ],
   "source": [
    "dict_lex = {}\n",
    "with open('./data/dict_lex.pkl', 'rb') as lex_file:\n",
    "    dict_lex = pickle.load(lex_file)\n",
    "\n",
    "print(len(dict_lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_lex(sent, dict_lex):\n",
    "    res = 0\n",
    "    for word in sent.strip().split():\n",
    "        if word.lower() in dict_lex:\n",
    "            res += dict_lex[word.lower()]\n",
    "    return res\n",
    "\n",
    "def extract_full_cap(sent):\n",
    "    res = 0\n",
    "    for word in sent.strip().split():\n",
    "        if len(word) > 1 and word.isupper() and \\\n",
    "            word not in ['<PERSON>', '<URL>', '<ADDRESS>', '<PHONE>']:\n",
    "            res += 1\n",
    "    return res\n",
    "\n",
    "def extract_char_cap(sent):\n",
    "    res = 0\n",
    "    for word in sent.strip().split():\n",
    "        if word not in ['<PERSON>', '<URL>', '<ADDRESS>', '<PHONE>']:\n",
    "            for char in word:\n",
    "                if char.isupper():\n",
    "                    res += 1\n",
    "    return res\n",
    "\n",
    "def extract_char_count(sent):\n",
    "    return len(sent)\n",
    "\n",
    "def extract_word_count(sent):\n",
    "    return len(sent.strip().split())\n",
    "\n",
    "def extract_punct(sent, punct):\n",
    "    res = 0\n",
    "    for char in sent:\n",
    "        if char in punct:\n",
    "            res += 1\n",
    "    return res\n",
    "\n",
    "def extract_elong(sent):\n",
    "    res = 0\n",
    "    for word in sent.strip().split():\n",
    "        if len(word) > 2 and word[-1] == word[-2] and word[-2] == word[-3]:\n",
    "            res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text     val     aro  \\\n",
      "1637                    internet is jackkked . love you  0.5625  0.2500   \n",
      "1585  jus chillin i came in second in da 400 and thi...  0.6250  0.1250   \n",
      "1213                   ... And boom goes the dynamite .  0.5000  0.1250   \n",
      "2779  i wish there was not any snow outside so i cou...  0.4375  0.0000   \n",
      "819   You can make what you don't understand mean an...  0.5625  0.0625   \n",
      "\n",
      "        lex  full_cap  char_cap  char_count  word_count  punct_ex  punct_qt  \\\n",
      "1637  0.917         0         0          31           6         0         0   \n",
      "1585 -0.006         0         0         127          28         0         0   \n",
      "1213  0.000         0         1          32           7         0         0   \n",
      "2779  0.000         0         0          54          12         0         0   \n",
      "819  -0.043         0         1          54          10         0         0   \n",
      "\n",
      "      elong_w  \n",
      "1637        0  \n",
      "1585        0  \n",
      "1213        1  \n",
      "2779        0  \n",
      "819         0  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_pickle('./data/df_train.pkl')\n",
    "df_train = df_train.assign(lex = df_train.apply(lambda row: extract_lex(row['text'], dict_lex), axis=1))\n",
    "df_train = df_train.assign(full_cap = df_train.apply(lambda row: extract_full_cap(row['text']), axis=1))\n",
    "df_train = df_train.assign(char_cap = df_train.apply(lambda row: extract_char_cap(row['text']), axis=1))\n",
    "df_train = df_train.assign(char_count = df_train.apply(lambda row: extract_char_count(row['text']), axis=1))\n",
    "df_train = df_train.assign(word_count = df_train.apply(lambda row: extract_word_count(row['text']), axis=1))\n",
    "df_train = df_train.assign(punct_ex = df_train.apply(lambda row: extract_punct(row['text'], '!'), axis=1))\n",
    "df_train = df_train.assign(punct_qt = df_train.apply(lambda row: extract_punct(row['text'], '%@#*&^?'), axis=1))\n",
    "df_train = df_train.assign(elong_w = df_train.apply(lambda row: extract_elong(row['text']), axis=1))\n",
    "print(df_train.head())\n",
    "df_train.drop(['text', 'val', 'aro'], inplace=True, axis=1)\n",
    "df_train.to_pickle('./data/df_train_norm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('./data/df_test.pkl')\n",
    "df_test = df_test.assign(lex = df_test.apply(lambda row: extract_lex(row['text'], dict_lex), axis=1))\n",
    "df_test = df_test.assign(full_cap = df_test.apply(lambda row: extract_full_cap(row['text']), axis=1))\n",
    "df_test = df_test.assign(char_cap = df_test.apply(lambda row: extract_char_cap(row['text']), axis=1))\n",
    "df_test = df_test.assign(char_count = df_test.apply(lambda row: extract_char_count(row['text']), axis=1))\n",
    "df_test = df_test.assign(word_count = df_test.apply(lambda row: extract_word_count(row['text']), axis=1))\n",
    "df_test = df_test.assign(punct_ex = df_test.apply(lambda row: extract_punct(row['text'], '!'), axis=1))\n",
    "df_test = df_test.assign(punct_qt = df_test.apply(lambda row: extract_punct(row['text'], '%@#*&^?'), axis=1))\n",
    "df_test = df_test.assign(elong_w = df_test.apply(lambda row: extract_elong(row['text']), axis=1))\n",
    "df_test.drop(['text', 'val', 'aro'], inplace=True, axis=1)\n",
    "df_test.to_pickle('./data/df_test_norm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, token_pattern='\\S+')\n",
    "re_emo = re.compile(\"([;:=][-\\\"']?[\\*P)(\\]><]+|[\\*P)(\\]<>]+[-\\\"']?[;:=]|lol|<3)\", re.IGNORECASE)\n",
    "\n",
    "def extract_emo(sent, re_emo):\n",
    "    return ' '.join(re_emo.findall(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('./data/df_train.pkl')\n",
    "df_train = df_train.assign(emo_tmp = df_train.apply(lambda row: extract_emo(row['text'].lower(), re_emo), axis=1))\n",
    "\n",
    "train_emo = vectorizer.fit_transform(df_train.emo_tmp)\n",
    "\n",
    "df_train_emo = pd.DataFrame(train_emo.todense())\n",
    "df_train_emo.set_index(df_train.index, inplace=True)\n",
    "df_train_emo.to_pickle('./data/df_train_emo.pkl')\n",
    "\n",
    "df_test = pd.read_pickle('./data/df_test.pkl')\n",
    "df_test = df_test.assign(emo_tmp = df_test.apply(lambda row: extract_emo(row['text'].lower(), re_emo), axis=1))\n",
    "\n",
    "test_emo = vectorizer.transform(df_test.emo_tmp)\n",
    "\n",
    "df_test_emo = pd.DataFrame(test_emo.todense())\n",
    "df_test_emo.set_index(df_test.index, inplace=True)\n",
    "df_test_emo.to_pickle('./data/df_test_emo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(df_train[df_train.index==1167])\n",
    "print(df_train_emo[df_train_emo.index==1167])\n",
    "print(df_test[df_test.index==1758])\n",
    "print(df_test_emo[df_test_emo.index==1758])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('./data/df_train.pkl')\n",
    "df_test = pd.read_pickle('./data/df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for index, row in df_train.iterrows():\n",
    "    sentences.append(gensim.models.doc2vec.TaggedDocument(row.text.split(), tags=[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2315\n",
      "Test set size: 579\n"
     ]
    }
   ],
   "source": [
    "# model = gensim.models.Doc2Vec(sentences, size=k, window=8, min_count=3, workers=4, iter=10)\n",
    "model = gensim.models.Doc2Vec(sentences, dm=0, dbow_words=1, size=100, window=10, hs=0,\n",
    "                              negative=5, sample=1e-4, iter=20, min_count=1, workers=4)\n",
    "\n",
    "df_train_doc2vec = pd.DataFrame(list(model.docvecs))\n",
    "df_train_doc2vec.drop([i for i in range(0, df_train_doc2vec.index.size) if i not in df_train.index],\n",
    "                      inplace=True)\n",
    "\n",
    "print(\"Train set size: %d\" % df_train_doc2vec.index.size)\n",
    "\n",
    "df_train_doc2vec.to_pickle('./data/df_train_doc2vec.pkl')\n",
    "\n",
    "test_doc2vec = {}\n",
    "for index, row in df_test.iterrows():\n",
    "    test_doc2vec[index] = model.infer_vector(row.text.split())\n",
    "\n",
    "df_test_doc2vec = pd.DataFrame.from_dict(test_doc2vec, orient='index')\n",
    "print(\"Test set size: %d\" %df_test_doc2vec.index.size)\n",
    "df_test_doc2vec.to_pickle('./data/df_test_doc2vec.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

\documentclass[11pt]{article}

\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{minted}
\usemintedstyle{emacs}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[table,x11names]{xcolor}

\usepackage[titletoc,title]{appendix}

\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage{lscape}

\hyphenation{}


\begin{document}

\title{Topic mining for short-text documents on a micro-blogging site with Doc2Vec and clustering and its application to influence mining}


% author names and affiliations
\author{Thuong-Hai Pham and Carlos F. Diez SÃ¡nchez\\
Faculty of Information and Communication Technology\\
University of Malta\\
Msida MSD 2080, Malta}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
In the era of data explosion, especially digital text generated by World Wide Web users, there is an increasing demand for techniques that automatically organise large collections of texts for further analysis and other processing tasks. One family of those techniques is called ``topic model". These techniques discover underlying topics from a given corpus with or without human intervention, in other word, supervised and unsupervised. This report is aimed to examine the traditional Latent Dirichlet Allocation (LDA) and a proposed method of combining Doc2Vec and clustering technique on the problem of topic mining. For practical evaluation, Twitter\footnote{https://twitter.com/} is chosen to do experiment on three approaches: standard LDA, author-topic model and our proposed approach. We also covers background behind each method and reports difficulties when attempting to make use of our topic mining results for topic-sensitive influencers mining task with real life data.
\end{abstract}

% no keywords

\section{Introduction}

Applying topic model for micro-blogging site is a very important task to enhance our understanding of the social network. One very successful technique and also being considered as state-of-the-art in unsupervised topic model is Latent Dirichlet Allocation (LDA)\cite{Blei2003}. Some applications were proposed by Zhao et al. (2011) \cite{zhao2011comparing} with the work of comparing Twitter and traditional media by LDA, or finding topic-sensitive influencers on Twitter by Weng et al. (2010) \cite{Weng2010}.

It is important to be noted that applying LDA directly on micro-blogging sites is considered to be not a trivial task yet more challenging problem. This occurs due to the nature of micro-blogging sites which is the limited length of each posting unit (e.g. tweets on Twitter have maximum 140 characters each). In addition, proposed LDA solution while trying to solve this problem have to make more assumptions (single-topic tweets...)\cite{zhao2011comparing} about the data itself rather than the bag-of-words (BOW) assumption from original LDA.

Therefore, we consider examining clustering method such as K-means to discover the underlying topic. The feature learning is done by Doc2Vec of Le \& Mikolov\cite{le2014distributed}, which is an adaptation of word2vec\cite{mikolov2013distributed}.

For the rest of this report, in section \ref{background}, we discuss about the mathematical background behind BOW assumption for topic model: the infinite exchangeability and De Finetti theorem. Thereafter, we revise LDA as a generative probabilistic model and its latent variables in section \ref{lda}. However, LDA does not work well when applying directly to short-text documents as tweets, we then consider two variants of LDA to solve this problem which are author-topic model (\ref{author_topic}) and Twitter-LDA (\ref{twitter_lda}). To end with the background and related works, the three groups of methods to evaluate topic models are also mentioned in section \ref{evaluation}. To the most important part, we figure out the disadvantages of these methods and present our proposal (section \ref{proposal}).
% TODO: add more


\section{Background} \label{background}


\subsection{Mathematical background} \label{math}

\subsubsection{Exchangeability}

We say that $(x_1,x_2...)$ is an infinitely exchangeable sequence of random variables if, for any $n$, the joint probability $p(x_1,x_2,...,x_n)$ is invariant to permutation of the indices. That is, for any permutation $\pi$,
\[p(x_1,...,x_n) = p(x_\pi(1),...,x_\pi(n))\]
It is important to emphasise that independent and identically distributed random variables are always infinitely exchangeable. However, infinite exchangeability is a much broader concept than being independent or identically distributed. 
For example, let $(x_1,x_2,\dots)$ be independent and identically distributed, and let $x_0$ be a non-trivial random variable independent of the rest. Then $(x_0+x_1,x_0+x_2,\dots)$ is infinitely exchangeable but not independent and identically distributed.

\subsubsection{De Finetti theorem, 1935}
A sequence of random variables $(x_1,x_2,...)$ is infinitely exchangeable iff, for all $n$,
\[p(x_1,x_2,...,x_n)=\int\prod_{i=1}^{n}p(x_i|\theta)P(d\theta)\]
for some measure $P$ on $\theta$.
If one assumes the data is infinitely exchangeable, then there must exist an underlying parameter and prior.

\subsection{Latent Dirichlet Allocation} \label{lda}

LDA is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where a topic is characterised by a distribution over words. To implement this idea, LDA assumes each document is a bag of words (BOW assumption). Hence, it applies infinite exchangeability on the documents and inherits the De Finetti theorem to expect an latent parameter and prior underlying in the corpus. These latent variables are illustrated in the figure \ref{fig:lda_model} below.


\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.3]{lda_model}
	\caption{LDA graphical model}
	\label{fig:lda_model}
\end{figure}

In figure \ref{fig:lda_model}:
\begin{itemize}
	\item $\alpha$ is Dirichlet distribution parameter, controls the shape and sparsity of $\theta$
	\item $\theta$ are per-document topic proportions.\\
	$\theta$ is a K-dimensional Dirichlet random variable, takes values in the (k-1)-simplex, and has the following probability density on this simplex:
	\[p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^{K}\alpha_i)}{\prod_{i=1}^{K}\Gamma(\alpha_i)}\theta_1^{\alpha_1-1}\dots\theta_K^{\alpha_K-1}\]
	The Dirichlet is conjugate to the multinomial. Given a multinomial observation, the posterior distribution of $\theta$ is a Dirichlet.
	\item $Z_{d,n}$ is per-word topic assignment, in which $D$ and $N$ are number of documents and number of words in a specific document, respectively.
	\item $W_{d,n}$ is observed word.
	\item $\beta$ are topics, which is $V$ dimensional Dirichlet.
	\item $\eta$ is the topic hyper parameter.
\end{itemize}

The blue-shaded node denotes observed variable, the others are hidden or latent variables. Plates denote replicated structures.

From a collection of documents, LDA infers: per-word topic assignment $Z_{d,n}$, per-document topic proportions $\theta_d$ and per-corpus topic distributions $\beta_k$.

\subsubsection{Generative process}

As mentioned above, LDA is a generative probabilistic model, which generative process is performed as described below:
\begin{enumerate}
	\item Draw $\theta_d \sim Dir(\alpha)$
	\item Draw $\beta_k \sim Dir(\eta)$
	\item For each of the N words in document d $W_{d,n}$:
	\begin{enumerate}
		\item Draw a topic $Z_{d,n} \sim Multinomial(\theta_d)$
		\item Draw a word $W_{d,n}$ from $p(W_{d,n}|Z_{d,n},\beta)$, a multinomial probability conditioned on the topic $Z_{d,n}$
	\end{enumerate}
\end{enumerate}

\subsubsection{Model inference}
However, in the real problem, to acquire underlying latent topics, we have to reverse the generative process by solving an inferential problem. The main goal of this inferential problem is to compute the posterior distribution of the latent variables in figure \ref{fig:lda_model}:
\[p(\theta,Z|W,\alpha,\beta)=\frac{p(\theta,Z,W|\alpha,\beta)}{p(W|\alpha,\beta)}\]

The function $p(\theta,Z|W,\alpha,\beta)$, in practice, is not possible to compute. Due to the conjugacy of Dirichlet distribution, we can marginalize over latent variables to rewrite the posterior $p(W|\alpha,\beta)$. This posterior is still hardly be inferred exactly. Nevertheless, there exist a wide variety of approximate inference algorithms for LDA:
\begin{itemize}
	\item Mean field variational methods \cite{blei2004variational} (Blei et al., 2001)
	\item Expectation propagation \cite{minka2002expectation} (Minka and Lafferty, 2002)
	\item Collapsed Gibbs sampling \cite{griffiths2004finding} (Griffiths and Steyvers, 2004)
	\item Collapsed variational inference \cite{teh2006collapsed} (Teh et al., 2006)
\end{itemize}

After being approximated, beside LDA, the posterior can be used in many other applications such as collaborative filtering, document similarity and information retrieval...

\subsection{Latent Dirichlet Allocation variants for Twitter} \label{lda_app}

One very basic approach is to apply LDA directly to Twitter by treating each tweet as a single document. However, due to the constraint of 140 characters per tweet, a tweet is too short for LDA to figure out the topic proportions.

\subsubsection{The author-topic model} \label{author_topic}

To overcome this issue, by excluding the topic proportions for each tweets but taking into consideration only the underlying topics in each user, aggregating all tweets of a Twitter's user into a single document was proposed and gained a better result to direct LDA \cite{Weng2010,hong2010empirical}.

On one hand, this approach is very efficient on a specific task (e.g. topic-sensitive influencers mining \cite{Weng2010}) by not modifying the inference process of original LDA. On the other hand, the target of this approach is each user, not tweets, so it is not applicable for a general problem of topic mining.

\subsubsection{Twitter-LDA} \label{twitter_lda}

On a different perspective, while attempting to compare Twitter and traditional media, Zhao et al. \cite{zhao2011comparing} proposed Twitter-LDA, a modified version of LDA to work on Twitter's short tweets without concatenating all tweets into one.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{twitter_lda_model}
	\caption{Twitter-LDA graphical model}
	\label{fig:twitter_lda_model}
\end{figure}

In figure \ref{fig:twitter_lda_model}, the author introduced four more variables:
\begin{itemize}
	\item $\beta^B$ denotes the background words distribution
	\item $\pi$ denotes a Bernoulli distribution which simulate the choice of authors between topic-related words and background words.
	\item $\gamma$ is the parameter of distribution $\pi$.
	\item $Y_{u,d,v}$ denotes the selection of background or topic word.
\end{itemize} 
and a slightly modification on $\theta$ that $\theta_u$ represents per-user topic proportions instead of per-document as in the original version. In addition, the D (document) plate is surrounded by a new plate U which stands for each user. It is necessary to mention that Twitter-LDA makes an assumption in which each tweet only conveys a single topic. We will clarify our disagreement on this assumption in section \ref{proposal}.

By defining the model as in figure \ref{fig:twitter_lda_model}, the generative process of Twitter-LDA is performed as followed:
\begin{enumerate}
	\item Draw $\beta^B \sim Dir(\eta)$
	\item Draw $\pi \sim Dir(\gamma)$
	\item Draw $\beta_k \sim Dir(\eta)$
	\item For each user,
	\begin{enumerate}
		\item Draw $Z_{u,d} \sim Multi(\theta_u)$
		\item For each word in document d,
		\begin{enumerate}
			\item Draw $Y_{u,d,n} \sim Multi(\pi)$
			\item Draw \[W_{u,d,n} \sim 
			\begin{cases}
			Multi(\theta^B) & \text{if $Y_{u,d,n} = 0$}\\
			Multi(\theta^{Z_{u,d}}) & \text{if $Y_{u,d,n} = 1$}
			\end{cases}\]
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

\subsection{Evaluation} \label{evaluation}

Wallach et al. \cite{Wallach2009a} summarised a variety of methods to evaluate LDA. As a topic model method, LDA is commonly evaluated by intrinsic and extrinsic evaluation. 

\subsubsection{Intrinsic evaluation}
One very basic intrinsic evaluation method is to view the problem as document modelling \cite{Blei2003}. By stating that, the goal of the model is to achieve high likelihood on a held-out test set, $C'$. In this case, the perplexity measure is used as in normal language modelling problem, in which the lower perplexity, the better performance the model achieves.
\[perp(C')=exp\left\{-\frac{\sum_{d=1}^{D}{log(p(W_d))}}{\sum_{d=1}^{D}N_d}\right\}\]

More advanced, measurement is also estimated by the probability of unseen held-out documents given some training documents. This probability can be written as \cite{Wallach2009a}
\[P(C|C')=\int d\theta d\alpha dm P(C|\theta,\alpha m)P(\theta,\alpha m|C').\]
in which, $C, C'$ denote training documents set (corpus) and held-out documents set, respectively. Noted that $m$ is the base measure of Dirichlet distribution, in addition to the concentration parameter $\alpha$.

There is also a variation of this method, document completion, which compare predictive performance by estimating the probability of the second half of each document given the first half. In this point of view, let $c^{(1)}$ be the first half and $c^{(2)}$ be the second half, the goal of our measurement is to compute
%\[P(w^{(2)}|w^{(1)},\theta,\alpha m)=\frac{P(w^{(2)}, w^{(1)}|\theta,\alpha m)}{P(w^{(1)}|\theta,\alpha m)}\]

\subsubsection{Extrinsic evaluation}
On the other hand, extrinsic approaches measure LDA performance on some secondary tasks, such as corpus comparison \cite{zhao2011comparing} or topic-sensitive influencers mining \cite{Weng2010}. These approaches are similar to the way language models performance are measured.

\subsubsection{Human evaluation}

As a part of the corpus comparing work \cite{zhao2011comparing}, Zhao et al. also evaluated the performance between original LDA, author-topic model and their proposed Twitter-LDA. Based on preliminary experiments, the authors set number of topic K to 110 for each model, then mixed 330 topics from the three models. The topics were then scored by two human judges. Each assigned a score on each topic, ranging from 1 (meaningful) to 0 (nonsense).

The result showed that Twitter-LDA gained 25.23\% higher in term of average score to author-topic model, and 32.61\% higher then standard LDA. Hence, Twitter-LDA obviously outperformed the two previous methods and were used in their comparison task.


\section{Proposed method} \label{proposal}

Both methods, author-topic model and Twitter-LDA, have overcome the problem of tweets' size in the micro-blogging site Twitter, which prevented the direct usage of the original LDA. More than that, it has been showed that Twitter-LDA outperforms the other two in capturing more meaningful topics. Nevertheless, Twitter-LDA has to change the original LDA process and inference approximation algorithm to implement its idea. This approach is hard to be re-implemented in the industrial sector by using existing libraries for other problems.

More than that, it is worth to consider the assumption of one tweet belongs only to one topic. One topic may represents only one topic intended by its Twitter user. However, please note that this topic is not the final topic discovered by our mining methods but may be a combination of the two final topics. For example, the topic user intend to tweet about is public insurance. Throughout the whole corpus, our mining process points out two distinct topics: health care and public administration. It is obvious that the user-intended topic reflects both of our discovered topics on the perspective of the whole corpus.

Bearing that in mind, we would like to propose a method to compare with original LDA and author-topic model, by accepting only BOW assumption.

\subsection{Clustering based on distributed representation of sentences} \label{doc2vec}

\subsubsection{Distributed representation of sentences for features learning}
After word2vec\cite{mikolov2013distributed}, Le and Mikolov presented the paragraph (document) vector models. Formally, the objective of a word embedding model  is to maximise the log probability
\[\frac{1}{T}\sum_{t=k}^{T-k}\log p(w_t|w_{t-k},\dots,w_{t+k}) \]
given a sequence of training words $w_1,w_2,w_3,\dots,w_T$.

% TODO: more about transition from word2vec to doc2vec

The paragraph vector models uses the same idea to develop the paragraph vector framework. In fact, this is not a single model but implemented into two different approaches: Distributed Memory model (DM) and Distributed Bag of Words (DBOW) - vector without word ordering.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{doc2vec}
	\caption{Framework for learning paragraph vector, (a) Distributed Memory, (b) Distributed Bag of Words}
	\label{fig:doc2vec}
\end{figure}

In figure \ref{fig:doc2vec} above\footnote{https://arxiv.org/pdf/1405.4053v2.pdf}, the DM model (a) is actually constructed from each word vector of that structure (sentence/document), then these vectors are combined (through averaging or concatenation) to learn the sentence/document features. In addition, a paragraph matrix is maintained to keep track of the whole sentence/document. In contrary to that, the DBOW model does not combine any word vectors yet only a paragraph vector is trained to predict the context.


\subsubsection{Clustering based on the learnt features}
Our proposed method consists of two distinguished phases. First, ``meaning" or, in fact, features of each document is learnt by Doc2Vec model as presented above. Having the features, the documents are then clustered using clustering technique. 

For the goal of our experiment described latter in \ref{evaluation}, we choose K-means to find hard clusters from our documents, each cluster represents a topic. However, to produce a probabilistic topic proportions as in LDA, we can easily change K-means to C-mean to achieve fuzzy or soft clusters.

% TODO: more about parameters used

\section{Experiment} \label{experiment}

\subsection{Data acquisition}
The data for this project was acquired from the Archive Team's Twitter Stream Grab (a collection in the JSON format collected from the general Twitter stream) in July 2016.\footnote{https://archive.org/details/archiveteam-twitter-stream-2016-07}  Although we could have streamed the data directly from Twitter's Streaming APIs, data preprocessing still plays an essential role to filter out tweets in other languages (Chinese, Japanese, Spanish...), remove stop-words, tokenise words within tweets, remove urls and normalise unconventional language used on social networks (character repetition, emoticons, etc.).

\subsection{Implementation}
The source-code used for the latter evaluation is developed using LDA and Doc2Vec model in Gensim\footnote{\url{https://radimrehurek.com/gensim/}} library.


\subsection{Resources} \label{resources}

Due to the magnitude of our data (48.7GB in compressed format), a sufficiently efficient machine is required to perform data preprocessing and calculation for our experiment. For this reason, we made use of the Compute Engine cloud n1-standard-4\footnote{operates with 4 virtual CPUs, 15GB RAM, 200GB hard disk drive}. In addition, the experiment evaluation process also required to employ 2 judges (the more, the better) to individually score meaningfulness of our topic mining result.

\subsection{Evaluation}

For the evaluation task, although perplexity is considered to justify the result with less subjectivity, it does not measure how meaningful the topics discovered are. Hence, we make use of the human evaluation strategy\cite{zhao2011comparing} instead. This evaluation process is performed by two distinct judges. Each judge assigns a name for every one of the topics in the output results, with a meaningfulness score on the scale from 0 to 10. Afterwards, their topic names and scores are exchanged and scored how much they agree with the other judge's evaluation in a similar scale from 0 to 10. The meaningfulness of each approach is measured by averaging all the agreement score above, which basically reflects the interpretability of the result.

\section{Topic mining results}
Each experiment divided 606631 tweets (from top 1600 users in case of separated) or 1600 documents by aggregating tweets from 1600 users (in case of author-topic) into 20 topics. The two judges scored each one of the topics according to 1) topic assignment, and 2) topic meaningfulness. Afterwards, we obtained the average for each one of the scores. The visualisation (obtained by applying t-SNE to 2 dimensions) of the four experiments in Figure \ref{fig:res_distribution} shows an intuitive impression that Doc2Vec methods have a better documents-topic distribution, in which documents of different topics are spread further from each other. The results in detail will be discussed further in the following part.
% Figure 4 shows a much better grouping of topics when the tweets were separated. The results will be discussed in detail in the next section.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{lda_sep}
    \caption{LDA (tweets separated)}\label{fig:res_lda_sep}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{doc_sep}
    \caption{Doc2Vec (tweets separated)}\label{fig:res_doc_sep}
    \end{subfigure}
    
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{lda_grp}
    \caption{LDA (author-topic)}\label{fig:res_lda_grp}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{doc_grp}
    \caption{Doc2Vec (author-topic)}\label{fig:res_doc_grp}
    \end{subfigure}
    
    \caption{Topics and documents (tweets) distributions}
    \label{fig:res_distribution}
\end{figure}

\begin{table}[H]
	\centering
	\begin{tabular}{| l | p{2.5cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} |}
		\hline
		& \textbf{LDA\newline (tweets separated)} & \textbf{LDA (author-topic)} & \textbf{Doc2Vec (tweets separated)} & \textbf{Doc2Vec (author-topic)}\\
		\hline
		\textbf{Avg. meaningfulness} & 3.025 & 1.275 & \cellcolor{blue!25}6.525 & 5.05\\
		\textbf{Avg. agreement} & 8.15 & 7.35 & \cellcolor{blue!25}8.2 & 7.725\\
		\hline
	\end{tabular}
	\caption{Topic mining results}
	\label{tb:res_meaningfulness}
\end{table}

In table \ref{tb:res_meaningfulness} above, LDA obtained the worst results in terms of averaged topic meaningfulness, 3.025 when the tweets were separated, and 1.275 when they were grouped as author-topic. On the other hand, Doc2Vec showed a much better averaged topic meaningfulness: 6.525 when the tweets were separated, and 5.050 when they were grouped as author-topic. 

The average score agreement between the two judges for all the experiments was 7.85625. The agreement was lower when assessing the author-topic methods with both, LDA (7.35), and Doc2Vec (7.725) methods, compared to the results when the tweets were separated (8.15 for LDA and 8.2 for Doc2Vec respectively). These results suggest that using the tweets separately is more meaningful than the author-topic method. This might be due to the fact that the topics were more disperse and difficult to determine for the judges when tweets were aggregated as a single document. One possible explanation is because ``normal" Twitter users tend to tweet many aspects of their lives which aspects are prevalent in a large number of users. Hence, these users are grouped together. While when tweets were separated, the topic mining methods can look into all tweets in the ``aspects" dimension instead of users dimension. Therefore, topics discovered are more clear and distinct.

Also from that observation, we draw a counter-argument for the author-topic LDA, when it did not obtain the results better than LDA as expected, and actually showed a significantly worse performance than the traditional LDA (without aggregation). Most of the 20 topics were not very informative, and showed little distance in the Intertopic Distance Map (Figure \ref{fig:res_trad_var}). We identified a couple of reasons: the first one is that usually most Twitter accounts might be talking of a wide variety of topics in their time-lines, instead of focusing on only one as discussed above. Another reason might be the span of the data is too short (one month), and therefore there was no space for more topics to be included.


\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{lda_vis_sep}
    \caption{Traditional LDA (tweets separated)}\label{fig:res_lda_vis_sep}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
    \includegraphics[width=\linewidth]{lda_vis_grp}
    \caption{Author-topic LDA (tweets aggregated)}\label{fig:res_lda_vis_grp}
    \end{subfigure}
    
    \caption{Traditional LDA \& author-topic model topic distribution visualised by LDAVis}
    \label{fig:res_trad_var}
\end{figure}

\subsection{Topic description with the four methods}

Let us take one example of the word topics shown in all the experiments to show how each method treated the corpus. The word topics 'shoes' (7 times) and 'size' (11 times) are some of the most frequent words in all the methods.   

In the traditional LDA (with the tweets separated), both words appear only in one topic (Topic 11), with words like `air', `jordan' and `nike', framing a very clear topic, the very popular sneakers in the 1990's. This is also what happens in the Doc2Vec method with the tweets separated.  

On the other hand, in the LDA using author-topic, the word topic `size' appears five times, in two topics (Topic 10 and 11), and both of them seem to talk about the same shoes, but the other three (Topics 3, 4 and 19) do not have any specific topic, as shown by the judge agreement topic and meaningfulness agreement. The same happens with the Doc2Vec using the author-topic method. The word 'size' appears in three topics (Topics 6, 11 and 18), with one of them not attached to any specific topic, as shown by the same judgement scores. 

\section{Influencers mining}

After having topics mined from our data, we attempt to implement a topic-sensitive influencers mining approach by combining PageRank and document-topic distribution.

\subsection{PageRank}
Page Rank was the first link analysis algorithm used by Google Search to rank websites in heir search engine results, by assigning a numerical weighting to each element of a hyperlinked set of documents. The underlying assumption is that the most important websites are more likely to receive more links from other websites, therefore, it counts the number and quality of links to estimate how important a website is.
% TODO: Write more about PageRank in concept

\subsection{Topic-sensitive PageRank}
Weng et al. (2010) used  TwitterRank and showed it outperforms other algorithms used to measure micro-blogging sites, including the original PageRank and the one used by Haveliwala (2002) \cite{haveliwala_2002}. TwitterRank basically measures the influence taking both the topical similarity between users and the link structure into account.
% TODO: present topic-sensitive PageRank

In Figure \ref{fig:topic_2_pagerank}, $\sum_{t=1}^{T}{DT_{d,t}} = 1, \forall 1\le d\le D$.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{topic_2_pagerank}
	\caption{Output from topic mining needed for PageRank}
	\label{fig:topic_2_pagerank}
\end{figure}

With the topic-user distribution matrix above, we then perform PageRank algorithm with a slightly modification. On each edge from node $i$ to node $j$ in the graph, we assign a weight of
\[P_t(i,j)=\frac{T_j}{\sum{a:s_i\to s_a|T_a|}}*sim_t(i,j)\]
in which $sim_t(i,j)=1-|DT_{i,t}-DT_{j,t}|$

\subsection{Difficulties \& future improvements}

However, while implementing this approach, we found challenging obstacles. Firstly, the Twitter REST API only allows us to query 15 user ids per a window of 15 minutes. That lead us to a solution of making queries in priority queue where user with the most tweeted tweets was queried first. By that approach, we successfully achieved followers for top 1600 users. Nevertheless, the second problem appeared at this stage, which can be described as follow,

\begin{listing}[H]
    \begin{minted}{python}
df_users = pd.read_pickle('./data/data_user_selected_full.pkl')
t1 = df_users.apply(lambda row: len(row['inner_group']), axis = 1)
t1.describe()
    count    1600.0
    mean        0.0
    std         0.0
    min         0.0
    25%         0.0
    50%         0.0
    75%         0.0
    max         0.0
    dtype: float64
t2 = df_users.apply(lambda row: len(row['outer_group']), axis = 1)
t2.describe()
    count    1.600000e+03
    mean     1.833375e+04
    std      5.810536e+04
    min      1.000000e+00
    25%      4.405000e+02
    50%      3.314000e+03
    75%      1.895000e+04
    max      1.398493e+06
    dtype: float64
    \end{minted}
\end{listing}

In the code snippet above, the number of followers for each users (outer\_group) are ranging from 1 to $1.4\times 10^6$ with an average of $2\times 10^4$. On the other hand, the number of followers that are in the group of 1600 (inner\_group) are always zero. This lead to the failure of PageRank, as by no followers in the inner group, our graph will be divided into 1600 strong-connected components. With these 1600 strong-connected components for each in user, our 1600 users have no influence on the others.

To overcome this problem, another data acquiring method should be performed instead. We suggest implementing breadth-first search (BFS) algorithm to re-connect our separated strong-connected components with starting vertexes are the 1600 users. Our BFS algorithm then expands our graph by tracing the followers of each follower of the starting user. This iteration keeps continuing until we significantly reduce the number of connected components and is expected to be performed in finite steps, backed up by the finite degree of separation on the social networks (3.5 from Facebook report\footnote{https://research.fb.com/three-and-a-half-degrees-of-separation/}).

\section{Conclusion}
From the experiments, results showed that our proposed method with Doc2Vec and clustering method (K-means) outperformed the traditional LDA and its variant of author-topic model in topic mining from random tweets on Twitter. Not only with the meaningfulness from discovered topics, our proposed method also removed the need of unnecessary assumptions on the data.

Also in this report, we found that author-topic LDA did not perform as well as traditional LDA, therefore, confirm our doubt about the assumption made by author-topic to make LDA work on short-text documents, especially on micro-blogging sites.

In contrary, our attempt to re-implement the TwitterRank methods has failed due to obstacles from the data. From that, we suggest an alternative data acquisition by breadth-first search on the graph of our data-ready users.

\bibliographystyle{IEEEtran}
\bibliography{report}

\begin{appendices}

\section{Topic mining raw results with LDA}

\begin{table}[H]
	\centering
	\begin{tabular}{| l | l | r | l | r | r | r | r | r |}
		\hline
		& \multicolumn{2}{c|}{\textbf{1st judge}} & \multicolumn{2}{c|}{\textbf{2nd judge}} & \multicolumn{2}{c|}{\textbf{Agreement}} & \multicolumn{2}{c|}{\textbf{Average}}\\
        \cline{2-9}
         & \textbf{Topic} & \textbf{Score} & \textbf{Topic} & \textbf{Score} & \textbf{1st} & \textbf{2nd} & \textbf{Score} & \textbf{Agree.}\\
		\hline
            0 & mtv event & 6 & mtv & 5 & 10 & 10 & 5.5 & 10\\
            1 & pics & 5 & NA & 0 & 8 & 7 & 2.5 & 7.5\\
            2 & people \& video & 0 & NA & 0 & 10 & 10 & 0 & 10\\
            3 & music \& numbers & 2 & NA & 0 & 10 & 8 & 1 & 9\\
            4 & phones & 6 & tech gadget & 8 & 9 & 9 & 7 & 9\\
            5 & us elections scandals & 7 & US election & 7 & 10 & 10 & 7 & 10\\
            6 & gaming & 4 & NA & 0 & 8 & 7 & 2 & 7.5\\
            7 & numbers & 0 & NA & 0 & 10 & 10 & 0 & 10\\
            8 & pokemon go & 3 & NA & 0 & 9 & 8 & 1.5 & 8.5\\
            9 & sales ads & 6 & online sales & 4 & 10 & 9 & 5 & 9.5\\
            10 & business & 4 & tech company & 7 & 7 & 8 & 5.5 & 7.5\\
            11 & shoes & 5 & shoes & 5 & 10 & 10 & 5 & 10\\
            12 & talk on ads & 0 & feelings & 1 & 9 & 6 & 0.5 & 7.5\\
            13 & sales & 0 & tech company & 4 & 8 & 6 & 2 & 7\\
            14 & motivational & 4 & feelings & 5 & 10 & 5 & 4.5 & 7.5\\
            15 & college & 3 & NA & 0 & 9 & 6 & 1.5 & 7.5\\
            16 & woman magazine & 7 & NA & 0 & 5 & 3 & 3.5 & 4\\
            17 & numbers \& news & 0 & NA & 0 & 10 & 2 & 0 & 6\\
            18 & news \& sales & 0 & NA & 0 & 10 & 5 & 0 & 7.5\\
            19 & news & 6 & politics & 7 & 9 & 6 & 6.5 & 7.5\\
		\hline
	\end{tabular}
	\caption{Scoring for LDA (tweets separated)}
	\label{tb:res_lda_sep}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{| l | l | r | l | r | r | r | r | r |}
		\hline
		& \multicolumn{2}{c|}{\textbf{1st judge}} & \multicolumn{2}{c|}{\textbf{2nd judge}} & \multicolumn{2}{c|}{\textbf{Agreement}} & \multicolumn{2}{c|}{\textbf{Average}}\\
        \cline{2-9}
         & \textbf{Topic} & \textbf{Score} & \textbf{Topic} & \textbf{Score} & \textbf{1st} & \textbf{2nd} & \textbf{Score} & \textbf{Agree.}\\
		\hline
            0 & sales & 5 & online shopping & 1 & 8 & 6 & 3 & 7\\
            1 & magazine \& sports & 0 & NA & 0 & 10 & 7 & 0 & 8.5\\
            2 & betting & 0 & NA & 0 & 10 & 7 & 0 & 8.5\\
            3 & shoes \& sex & 0 & NA & 0 & 10 & 9 & 0 & 9.5\\
            4 & toys in ebay & 2 & NA & 0 & 10 & 8 & 1 & 9\\
            5 & offers & 2 & online sales & 2 & 10 & 9 & 2 & 9.5\\
            6 & offers & 1 & NA & 0 & 10 & 4 & 0.5 & 7\\
            7 & numbers & 0 & NA & 0 & 10 & 8 & 0 & 9\\
            8 & gaming \& numbers & 0 & NA & 0 & 10 & 7 & 0 & 8.5\\
            9 & tutti-frutti news & 0 & mtv & 1 & 10 & 3 & 0.5 & 6.5\\
            10 & shoes & 1 & shoes & 2 & 10 & 9 & 1.5 & 9.5\\
            11 & shoes \& music & 0 & shoes & 2 & 10 & 7 & 1 & 8.5\\
            12 & news agency & 1 & NA & 0 & 10 & 2 & 0.5 & 6\\
            13 & football & 6 & NA & 0 & 7 & 2 & 3 & 4.5\\
            14 & forecast & 5 & weather & 4 & 10 & 9 & 4.5 & 9.5\\
            15 & offers & 3 & NA & 0 & 9 & 4 & 1.5 & 6.5\\
            16 & stock market news & 5 & NA & 0 & 8 & 2 & 2.5 & 5\\
            17 & housing & 4 & NA & 0 & 8 & 0 & 2 & 4\\
            18 & lego? & 0 & NA & 0 & 10 & 0 & 0 & 5\\
            19 & sports ads & 4 & NA & 0 & 9 & 2 & 2 & 5.5\\
		\hline
	\end{tabular}
	\caption{Scoring for LDA (author-topic)}
	\label{tb:res_lda_grp}
\end{table}

\section{Topic mining raw results with Doc2Vec \& K-means}

\begin{table}[H]
	\centering
	\begin{tabular}{| l | p{2.7cm} | r | p{2.7cm} | r | r | r | r | r |}
		\hline
		& \multicolumn{2}{c|}{\textbf{1st judge}} & \multicolumn{2}{c|}{\textbf{2nd judge}} & \multicolumn{2}{c|}{\textbf{Agreement}} & \multicolumn{2}{c|}{\textbf{Average}}\\
        \cline{2-9}
         & \textbf{Topic} & \textbf{Score} & \textbf{Topic} & \textbf{Score} & \textbf{1st} & \textbf{2nd} & \textbf{Score} & \textbf{Agree.}\\
		\hline
            0 & euro2016 ads & 6 & NA & 0 & 7 & 3 & 3 & 5\\
            1 & numbers & 0 & NA & 0 & 10 & 10 & 0 & 10\\
            2 & us election & 10 & US election & 10 & 10 & 10 & 10 & 10\\
            3 & mtv event \& lady gaga & 7 & mtv & 4 & 9 & 9 & 5.5 & 9\\
            4 & marketing, tech \& media & 8 & entrepreneur \& startup & 10 & 8 & 7 & 9 & 7.5\\
            5 & a news scandal & 7 & family & 3 & 6 & 6 & 5 & 6\\
            6 & sport shoes & 8 & shoes & 10 & 10 & 10 & 9 & 10\\
            7 & international news & 6 & world news & 9 & 9 & 10 & 7.5 & 9.5\\
            8 & shopping & 7 & online sale & 8 & 10 & 7 & 7.5 & 8.5\\
            9 & expensive shopping & 9 & jewelry & 7 & 10 & 8 & 8 & 9\\
            10 & magazine articles on sexual life & 8 & NA & 0 & 5 & 5 & 4 & 5\\
            11 & phones & 10 & tech gadget & 9 & 10 & 8 & 9.5 & 9\\
            12 & forecast & 8 & weather & 4 & 9 & 9 & 6 & 9\\
            13 & love & 8 & feelings & 5 & 9 & 7 & 6.5 & 8\\
            14 & tennagers stuff & 4 & life & 1 & 7 & 5 & 2.5 & 6\\
            15 & us sports & 10 & sports & 7 & 9 & 8 & 8.5 & 8.5\\
            16 & womens clothes & 9 & fashion & 8 & 10 & 9 & 8.5 & 9.5\\
            17 & job & 8 & business & 7 & 9 & 6 & 7.5 & 7.5\\
            18 & other sports & 8 & sports & 5 & 9 & 7 & 6.5 & 8\\
            19 & download video \& music & 6 & multimedia & 7 & 10 & 8 & 6.5 & 9\\
		\hline
	\end{tabular}
	\caption{Scoring for Doc2Vec \& K-means (tweets separated)}
	\label{tb:res_doc_sep}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{| l | p{2.7cm} | r | p{2.7cm} | r | r | r | r | r |}
		\hline
		& \multicolumn{2}{c|}{\textbf{1st judge}} & \multicolumn{2}{c|}{\textbf{2nd judge}} & \multicolumn{2}{c|}{\textbf{Agreement}} & \multicolumn{2}{c|}{\textbf{Average}}\\
        \cline{2-9}
         & \textbf{Topic} & \textbf{Score} & \textbf{Topic} & \textbf{Score} & \textbf{1st} & \textbf{2nd} & \textbf{Score} & \textbf{Agree.}\\
		\hline
        0 & shopping & 6 & online shopping & 7 & 10 & 9 & 6.5 & 9.5\\
        1 & mtv event & 6 & mtv & 5 & 10 & 10 & 5.5 & 10\\
        2 & jobs & 2 & business & 7 & 6 & 5 & 4.5 & 5.5\\
        3 & sexual scandal & 4 & sex \& life & 2 & 9 & 8 & 3 & 8.5\\
        4 & music \& tech & 6 & multimedia & 7 & 9 & 7 & 6.5 & 8\\
        5 & love music & 7 & feelings & 5 & 8 & 6 & 6 & 7\\
        6 & shoes \& phones & 0 & online shopping & 7 & 4 & 6 & 3.5 & 5\\
        7 & news scandal & 7 & family & 2 & 6 & 3 & 4.5 & 4.5\\
        8 & politics news & 6 & politics & 9 & 9 & 9 & 7.5 & 9\\
        9 & african dontknowwhat & 0 & porn & 6 & 6 & 7 & 3 & 6.5\\
        10 & writers & 3 & literature & 4 & 10 & 9 & 3.5 & 9.5\\
        11 & expensive shopping & 8 & fashion & 9 & 9 & 8 & 8.5 & 8.5\\
        12 & mainly us news & 6 & world news & 6 & 9 & 7 & 6 & 8\\
        13 & us elections & 8 & US election & 10 & 10 & 10 & 9 & 10\\
        14 & fancy writing style & 0 & NA & 0 & 10 & 7 & 0 & 8.5\\
        15 & numbers & 0 & NA & 0 & 10 & 10 & 0 & 10\\
        16 & women magazine talk & 8 & feelings & 5 & 7 & 4 & 6.5 & 5.5\\
        17 & ads & 5 & NA & 0 & 8 & 4 & 2.5 & 6\\
        18 & shopping items & 7 & sale & 4 & 9 & 6 & 5.5 & 7.5\\
        19 & marketing & 8 & tech entrepreneur \& startup & 10 & 8 & 7 & 9 & 7.5\\
		\hline
	\end{tabular}
	\caption{Scoring for Doc2Vec \& K-means (author-topic)}
	\label{tb:res_doc_grp}
\end{table}

\end{appendices}


\end{document}
